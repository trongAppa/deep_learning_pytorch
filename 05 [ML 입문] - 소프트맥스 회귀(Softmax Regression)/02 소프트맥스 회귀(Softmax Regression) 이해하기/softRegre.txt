소프트맥스 회귀를 통해 3개 이상의 선택지 중에서 1개를 고르는 다중 클래스 분류
1) 다중 클래스 분류(Multi-class Classification)
- 세 개 이상의 답 중 하나를 고르는 문제
y`는 예측값이라는 의미를 가지고 있으므로 가설식에서 h(x) 대신 사용되기도 합니다.

1. 로지스틱 회귀
시그모이드 함수는 예측값을 0과 1 사이의 값으로 만듬
두 확률의 총 합은 1
가설 : H(X) = sigmoid(WX + B) (logi.png)

2. 소프트맥스 회귀
확률의 총 합이 1이 되는 이 아이디어를 다중 클래스 분류 문제에 적용
각 선택지마다 소수 확률을 할당
총 확률의 합은 1
각 선택지가 정답일 확률로 표현
선택지의 개수만큼의 차원을 가지는 벡터를 만들고, 해당 벡터가 벡터의 모든 원소의 합이 1이 되도록 원소들의 값을 변환시키는 어떤 함수를 지나게 만들어야 함
가설 : H(X) = softmax(WX + B) (softmax.png)

2) 소프트맥스 함수(Softmax function)
분류해야하는 정답지(클래스)의 총 개수를 k라고 할 때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정

1. 소프트맥스 함수의 이해
k차원의 벡터에서 i번째 원소를 zᵢ, i번째 클래스가 정답일 확률을 pᵢ로 나타낸다고 하였을 때 소프트맥스 함수는 pᵢ를 다음과 같이 정의
p_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{k} e^{z_{j}}}\ \ for\ i=1, 2, ... k

풀어야하는 문제의 경우 k=3이므로 3차원 벡터 z=[z₁+z₂+z₃]의 입력을 받으면 소프트맥스 함수는 아래와 같은 출력을 리턴
softmax(z)=[\frac{e^{z_{1}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{2}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{3}}}{\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = \hat{y} = \text{예측값}

p₁+p₂+p₃ 각각은 1번 클래스가 정답일 확률, 2번 클래스가 정답일 확률, 3번 클래스가 정답일 확률을 나타내며 각각 0과 1사이의 값으로 총 합은 1

분류하고자 하는 클래스가 k개일 때, k차원의 벡터를 입력받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴한다는 내용을 식으로 기재

2. 그림을 통한 이해
살 붙일 그림(softmaxFunc1.png)

소프트맥스 함수의 입력
소프트맥스 함수의 입력 벡터 z의 차원수만큼 결과값의 나오도록 가중치 곱을 진행
하나의 샘플 데이터는 4개의 독립 변수 x를 가지는데 이는 모델이 4차원 벡터를 입력으로 받음을 의미
소프트맥스의 함수의 입력으로 사용되는 벡터는 벡터의 차원이 분류하고자 하는 클래스의 개수가 되어야 하므로 어떤 가중치 연산을 통해 3차원 벡터로 변환
소프트맥스 함수의 입력으로 사용되는 3차원 벡터를 z로 표현(softmaxFunc2.png)

소프트맥스 함수의 입력 벡터로 차원을 축소하는 방법은 간단
소프트맥스 함수의 입력 벡터 z의 차원수만큼 결과값의 나오도록 가중치 곱을 진행
그림(softmaxFunc2.png)에서 화살표는 총 (4 × 3 = 12) 12개이며 전부 다른 가중치를 가지고, 학습 과정에서 점차적으로 오차를 최소화하는 가중치로 값이 변경

오차 계산 방법
소프트맥스 함수의 출력은 분류하고자하는 클래스의 개수만큼 차원을 가지는 벡터로 각 원소는 0과 1사이의 값을 보유
각각은 특정 클래스가 정답일 확률
예측값과 비교를 할 수 있는 실제값의 표현 방법 존재해야됨
소프트맥스 회귀에서는 실제값을 원-핫 벡터로 표현(softmaxFunc3.png)
원-핫 인코딩을 수행하여 실제값을 원-핫 벡터로 수치화(softmaxFunc4.png)









