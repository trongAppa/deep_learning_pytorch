-- 벡터, 행렬 그리고 텐서(Vector, Matrix and Tensor) - 딥 러닝을 위한 가장 기본적인 수학적 지식인 벡터, 행렬, 텐서
벡터 : 1차원 텐서 / 행렬 : 2차원 텐서

-- 넘파이 훑어보기(Numpy Review) - 파이토치는 파이썬 패키지 넘파이(Numpy)와 유사
In [1]: import numpy as np

1) 1D with Numpy
In [2]:
t = np.array([0., 1., 2., 3., 4., 5., 6.])
# 파이썬으로 설명하면 List를 생성해서 np.array로 1차원 array로 변환함.
print(t)

Out[2]: [0. 1. 2. 3. 4. 5. 6.]

ndim - 차원 출력
In [3]: print('Rank of t: ', t.ndim)
Out[3]: Rank of t:  1

shape - 크기 출력
In [4]: print('Shape of t: ', t.shape)
Out[4]: Shape of t:  (7,)

1-1) Numpy 기초 이해하기
--Numpy에서 인덱스는 0부터 시작
In [5]: print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1]) # 인덱스를 통한 원소 접근
Out[5]: t[0] t[1] t[-1] =  0.0 1.0 6.0

In [6]: print('t[2:5] t[4:-1]  = ', t[2:5], t[4:-1]) # [시작 번호 : 끝 번호]로 범위 지정을 통해 가져온다.
Out[6]: t[2:5] t[4:-1]  =  [2. 3. 4.] [4. 5.]

In [7]: print('t[:2] t[3:]     = ', t[:2], t[3:]) # 시작 번호를 생략한 경우와 끝 번호를 생략한 경우
Out[7]: t[:2] t[3:]     =  [0. 1.] [3. 4. 5. 6.]

2) 2D with Numpy
In [8]: 
t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])
print(t)

Out[8]: 
[[ 1.  2.  3.]
 [ 4.  5.  6.]
 [ 7.  8.  9.]
 [10. 11. 12.]]
 
In [9]: 
print('Rank  of t: ', t.ndim)
print('Shape of t: ', t.shape)

Out[9]: 
Rank  of t:  2
Shape of t:  (4, 3)

-- 파이토치 텐서 선언하기(PyTorch Tensor Allocation)
-- torch 설치
pip install torch

In [1]: import torch

1) 1D with PyTorch
In [2]: t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])

In [3]: print(t)
Out[3]: tensor([0., 1., 2., 3., 4., 5., 6.])

In [4]:
print(t.dim())  # rank. 즉, 차원
print(t.shape)  # shape
print(t.size()) # shape

Out[4]: 
1
torch.Size([7])
torch.Size([7])

In [5]: 
print(t[0], t[1], t[-1])  # 인덱스로 접근
print(t[2:5], t[4:-1])    # 슬라이싱
print(t[:2], t[3:])       # 슬라이싱

Out[5]:
tensor(0.) tensor(1.) tensor(6.)
tensor([2., 3., 4.]) tensor([4., 5.])
tensor([0., 1.]) tensor([3., 4., 5., 6.])

2) 2D with PyTorch
In [6]: 
t = torch.FloatTensor([[1., 2., 3.],
					   [4., 5., 6.],
   					   [7., 8., 9.],
					   [10., 11., 12.]
					  ])
print(t)

Out[6]:
tensor([[ 1.,  2.,  3.],
        [ 4.,  5.,  6.],
        [ 7.,  8.,  9.],
        [10., 11., 12.]])
		
In [7]: 
print(t.dim())  # rank. 즉, 차원
print(t.size()) # shape
   
Out[7]:
2
torch.Size([4, 3])

In [8]:
print(t[:, 1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져온다.
print(t[:, 1].size()) # ↑ 위의 경우의 크기

Out[8]:
tensor([ 2.,  5.,  8., 11.])
torch.Size([4])

In [9]:
print(t[:, :-1]) # 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져온다.

Out[9]:
tensor([[ 1.,  2.],
        [ 4.,  5.],
        [ 7.,  8.],
        [10., 11.]])

3) 브로드캐스팅(Broadcasting)
In [10]:
m1 = torch.FloatTensor([[3, 3]])
m2 = torch.FloatTensor([[2, 2]])
print(m1 + m2)

Out[10]:
tensor([[5., 5.]])

In [11]:
# Vector + scalar
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([3]) # [3] -> [3, 3]
print(m1 + m2)

Out[11]:
tensor([[4., 5.]])

In [12]:
# 2 x 1 Vector + 1 x 2 Vector
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([[3], [4]])
print(m1 + m2)

Out[12]:
tensor([[4., 5.],
        [5., 6.]])
		
브로드캐스팅 과정

# 브로드캐스팅 과정에서 실제로 두 텐서가 어떻게 변화
[1, 2]
==> [[1, 2],
     [1, 2]]
[3]
[4]
==> [[3, 3],
     [4, 4]]
	 
주의 : 브로드캐스팅은 자동으로 수행되므로 사용자는 나중에 원하는 결과가 나오지 않았더라도 어디서 문제가 발생했는지 찾기가 굉장히 어려울 수 있습니다.

-- 자주 사용되는 기능들
1) 행렬 곱셈과 곱셈의 차이(Matrix Multiplication Vs. Multiplication)
-- matmul() 행렬 곱셈
In [13]:
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1.matmul(m2)) # 2 x 1

Out[13]:
Shape of Matrix 1:  torch.Size([2, 2])
Shape of Matrix 2:  torch.Size([2, 1])
tensor([[ 5.],
        [11.]])
		
-- mul() 원소별 곱셈에 사용 대신해서 * 사용가능
In [14]:
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape) # 2 x 2
print('Shape of Matrix 2: ', m2.shape) # 2 x 1
print(m1 * m2) # 2 x 2
print(m1.mul(m2))

Out[14]:
Shape of Matrix 1:  torch.Size([2, 2])
Shape of Matrix 2:  torch.Size([2, 1])
tensor([[1., 2.],
        [6., 8.]])
tensor([[1., 2.],
        [6., 8.]])

# 브로드캐스팅 과정에서 m2 텐서가 어떻게 변경과정
[1]
[2]
==> [[1, 1],
     [2, 2]]

2) 평균(Mean)
-- .mean() 행렬값의 평균 구하는 법
In [14]:
t = torch.FloatTensor([1, 2])
print(t.mean())

Out[14]: tensor(1.5000)

-- 2차원도 가능
In [15]:
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)

Out[15]:
tensor([[1., 2.],
        [3., 4.]])
		
In [16]: print(t.mean())

Out[16]: tensor(2.5000)

-- dim=? (?-1)차원을 인자로 평균 구함
In [16]: print(t.mean(dim=0)) # dim=0 첫번째 차원을 뜻함 / 열의 평균

Out[16]: tensor([2., 3.])

# 실제 연산 과정
t.mean(dim=0)은 입력에서 첫번째 차원을 제거한다.

[[1., 2.],
 [3., 4.]]

1과 3의 평균을 구하고, 2와 4의 평균을 구한다.
결과 ==> [2., 3.]

In [16]: print(t.mean(dim=1))

Out[16]: tensor([1.5000, 3.5000])

# 실제 연산 과정
t.mean(dim=0)은 입력에서 첫번째 차원을 제거한다.

[[1., 2.],
 [3., 4.]]

1과 2의 평균을 구하고, 3와 4의 평균을 구한다.
결과 ==> [1.5000, 3.5000]

3) 덧셈(Sum)
In [17]: 
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)

Out[17]: 
tensor([[1., 2.],
        [3., 4.]])

In [18]: 
print(t.sum()) # 단순히 원소 전체의 덧셈을 수행
print(t.sum(dim=0)) # 행을 제거
print(t.sum(dim=1)) # 열을 제거
print(t.sum(dim=-1)) # 열을 제거

Out[18]: 
tensor(10.)
tensor([4., 6.])
tensor([3., 7.])
tensor([3., 7.])

4) 최대(Max)와 아그맥스(ArgMax)
In [19]: 
t = torch.FloatTensor([[1, 2], [3, 4]])
print(t)

Out[19]: 
tensor([[1., 2.],
        [3., 4.]])

In [20]: print(t.max()) # Returns one value: max

Out[20]: tensor(4.)

In [21]: print(t.max(dim=0)) # Returns two values: max and argmax

Out[21]: 
torch.return_types.max(
values=tensor([3., 4.]),
indices=tensor([1, 1]))

# indices=tensor([1, 1])가 무슨 의미인지 봅시다. 기존 행렬을 다시 상기해봅시다.
[[1, 2],
 [3, 4]]
첫번째 열에서 0번 인덱스는 1, 1번 인덱스는 3입니다.
두번째 열에서 0번 인덱스는 2, 1번 인덱스는 4입니다.
다시 말해 3과 4의 인덱스는 [1, 1]입니다.

In [22]: 
print('Max: ', t.max(dim=0)[0])
print('Argmax: ', t.max(dim=0)[1])

Out[22]: 
Max:  tensor([3., 4.])
Argmax:  tensor([1, 1])

In [23]: 
torch.return_types.max(
values=tensor([2., 4.]),
indices=tensor([1, 1]))
torch.return_types.max(
values=tensor([2., 4.]),
indices=tensor([1, 1]))

Out[23]: tensor(4.)

5) 뷰(View) - 원소의 수를 유지하면서 텐서의 크기 변경. 매우 중요!
In [24]:
t = np.array([[[0, 1, 2],
               [3, 4, 5]],
              [[6, 7, 8],
               [9, 10, 11]]])
ft = torch.FloatTensor(t)

In [25]: print(ft.shape)

Out[25]: torch.Size([2, 2, 3])

5-1) 3차원 텐서에서 2차원 텐서로 변경
In [26]:
print(ft.view([-1, 3])) # ft라는 텐서를 (?, 3)의 크기로 변경
print(ft.view([-1, 3]).shape)

Out[26]: 
tensor([[ 0.,  1.,  2.],
        [ 3.,  4.,  5.],
        [ 6.,  7.,  8.],
        [ 9., 10., 11.]])
torch.Size([4, 3])

-- 규칙
# view는 기본적으로 변경 전과 변경 후의 텐서 안의 원소의 개수가 유지되어야 합니다.
# 파이토치의 view는 사이즈가 -1로 설정되면 다른 차원으로부터 해당 값을 유추합니다.

내부적으로 크기 변환은 다음과 같이 이루어졌습니다. (2, 2, 3) -> (2 × 2, 3) -> (4, 3)

5-2) 3차원 텐서의 크기 변경
In [27]:
print(ft.view([-1, 1, 3]))
print(ft.view([-1, 1, 3]).shape)

Out[27]:
tensor([[[ 0.,  1.,  2.]],

        [[ 3.,  4.,  5.]],

        [[ 6.,  7.,  8.]],

        [[ 9., 10., 11.]]])
torch.Size([4, 1, 3])

6) 스퀴즈(Squeeze) - 1인 차원을 제거한다.
In [28]:
ft = torch.FloatTensor([[0], [1], [2]])
print(ft)
print(ft.shape)

Out[28]:
tensor([[0.],
        [1.],
        [2.]])
torch.Size([3, 1])

In [29]:
print(ft.squeeze())
print(ft.squeeze().shape)

Out[29]:
tensor([0., 1., 2.])
torch.Size([3])

7) 언스퀴즈(Unsqueeze) - 특정 위치에 1인 차원을 추가한다.
In [30]:
ft = torch.Tensor([0, 1, 2])
print(ft.shape)

Out[30]:
torch.Size([3])

In [31]:
print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.
print(ft.unsqueeze(0).shape)

Out[31]:
tensor([[0., 1., 2.]])
torch.Size([1, 3])

In [32]:
print(ft.view(1, -1))
print(ft.view(1, -1).shape)

Out[32]:
tensor([[0., 1., 2.]])
torch.Size([1, 3])

In [33]:
print(ft.unsqueeze(1))
print(ft.unsqueeze(1).shape)

Out[33]:
tensor([[0.],
        [1.],
        [2.]])
torch.Size([3, 1])

In [34]:
print(ft.unsqueeze(-1))
print(ft.unsqueeze(-1).shape)

Out[34]:
tensor([[0.],
        [1.],
        [2.]])
torch.Size([3, 1])

# view(), squeeze(), unsqueeze()는 텐서의 "원소 수를 그대로 유지"하면서 모양과 차원을 조절합니다.

8) 타입 캐스팅(Type Casting) - 텐서 자료형
- 텐서_자료형.png 이미지 참조
In [35]:
lt = torch.LongTensor([1, 2, 3, 4])
print(lt)

Out[35]: tensor([1, 2, 3, 4])

In [36]: print(lt.float())

Out[36]: tensor([1., 2., 3., 4.])

In [37]:
bt = torch.ByteTensor([True, False, False, True])
print(bt)

Out[37]: tensor([1, 0, 0, 1], dtype=torch.uint8)

In [38]:
print(bt.long())
print(bt.float())

Out[38]:
tensor([1, 0, 0, 1])
tensor([1., 0., 0., 1.])

9) 연결하기(concatenate)
In [39]:
x = torch.FloatTensor([[1, 2], [3, 4]])
y = torch.FloatTensor([[5, 6], [7, 8]])

-- cat() 행렬 연결
In [40]: print(torch.cat([x, y], dim=0))

Out[40]:
tensor([[1., 2.],
        [3., 4.],
        [5., 6.],
        [7., 8.]])
		
In [41]: print(torch.cat([x, y], dim=1))

Out[41]:
tensor([[1., 2., 5., 6.],
        [3., 4., 7., 8.]])

10) 스택킹(Stacking)
In [42]:
x = torch.FloatTensor([1, 4])
y = torch.FloatTensor([2, 5])
z = torch.FloatTensor([3, 6])

-- stack() 쌓는다
In [43]: print(torch.stack([x, y, z]))

Out[43]:
tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])

In [44]: print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0)) # 43 stack을 unsqueeze와 cat을 사용하여 구현

Out[44]:
tensor([[1., 4.],
        [2., 5.],
        [3., 6.]])

11) ones_like와 zeros_like - 0으로 채워진 텐서와 1로 채워진 텐서
In [45]:
x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])
print(x)

Out[45]:
tensor([[0., 1., 2.],
        [2., 1., 0.]])

In [46]: print(torch.ones_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 1로 채우기

Out[46]:
tensor([[1., 1., 1.],
        [1., 1., 1.]])
		
In [47]: print(torch.zeros_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 0으로 채우기

Out[47]:
tensor([[0., 0., 0.],
        [0., 0., 0.]])
		
12) In-place Operation (덮어쓰기 연산)
In [48]: x = torch.FloatTensor([[1, 2], [3, 4]])

In [49]:
print(x.mul(2.)) # 곱하기 2를 수행한 결과를 출력
print(x) # 기존의 값 출력

Out[49]:
tensor([[2., 4.],
        [6., 8.]])
tensor([[1., 2.],
        [3., 4.]])

_ 붙이면 덮어쓰기함
In [50]:
print(x.mul_(2.))  # 곱하기 2를 수행한 결과를 변수 x에 값을 저장하면서 결과를 출력
print(x) # X 출력

Out[50]:
tensor([[2., 4.],
        [6., 8.]])
tensor([[2., 4.],
        [6., 8.]])

-- 참고
- 데이터사이언스 분야 한정으로 3차원 이상의 텐서는 그냥 다차원 행렬 또는 배열로 간주할 수 있습니다. 또한 주로 3차원 이상을 텐서라고 하긴 하지만, 1차원 벡터나 2차원인 행렬도 텐서라고 표현하기도 합니다. 같은 표현입니다. 
- 훈련 데이터 하나의 크기를 256이라고 해봅시다. [3, 1, 2, 5, ...] 이런 숫자들의 나열이 256의 길이로 있다고 상상하면됩니다. 다시 말해 훈련 데이터 하나 = 벡터의 차원은 256입니다. 만약 이런 훈련 데이터의 개수가 3000개라고 한다면, 현재 전체 훈련 데이터의 크기는 3,000 × 256입니다. 행렬이니까 2D 텐서네요. 3,000개를 1개씩 꺼내서 처리하는 것도 가능하지만 컴퓨터는 훈련 데이터를 하나씩 처리하는 것보다 보통 덩어리로 처리합니다. 3,000개에서 64개씩 꺼내서 처리한다고 한다면 이 때 batch size를 64라고 합니다. 그렇다면 컴퓨터가 한 번에 처리하는 2D 텐서의 크기는 (batch size × dim) = 64 × 256입니다.
- 텐서의 크기(shape)를 표현할 때는 ,(컴마)를 쓰기도 하고 ×(곱하기)를 쓰기도 합니다. 예를 들어 2행 3열의 2D 텐서를 표현할 때 (2, 3)라고 하기도 하고 (2 × 3)이라고 하기도 합니다. (5, )의 형식은 (1 × 5)를 의미합니다.
- 훈련 데이터의 개수가 굉장히 많을 때, 컴퓨터가 한 번에 들고가서 처리할 양을 배치 크기(batch size)라고 합니다.
- 슬라이싱에 대한 다른 예제 : https://wikidocs.net/13
- Pytorch 텐서에 대한 다양한 예제 : https://datascienceschool.net/view-notebook/4f3606fd839f4320a4120a56eec1e228/