1) 미니 배치와 배치 크기(Mini Batch and Batch Size)
- 전체의 데이터를 더 작은 단위로 나누어서 해당 단위로 학습하는 개념을 미니 배치
- 미니 배치를 학습시 매니 배치만큼만 가져가서 미니 배치에 대한 비용(cost)을 계산하고, 경사 하강법을 수행한다.
- 에포크(Epoch)는 전체 훈련 데이터가 학습에 한 번 사용된 주기
- 미니 배치의 개수는 결국 미니 배치의 크기를 몇으로 하느냐에 따라서 달라지는데 미니 배치의 크기를 배치 크기(batch size)
- 전체 데이터에 대해서 한 번에 경사 하강법을 수행하는 방법을 배치 경사 하강법
- 미니 배치 단위로 경사 하강법을 수행하는 방법을 미니 배치 경사 하강법
- 배치 경사 하강법은 과정은 안정적이지만 계산량이 많이 발생
- 미니 배치 경사 하강법은 값은 조금 애매해도 훈련 속도는 빠름

2) 이터레이션(Iteration)
- Iteration.png를 참조필요
- 한 번의 에포크 내에서 이루어지는 매개변수인 가중치 W와 b의 업데이트 횟수
- 전체 데이터가 2,000일 경우 배치 크기를 200으로 한다면 이터레이션 수는 10
- 한 번의 에포크 당 매개변수 업데이트가 10번 이루어짐을 의미

3) 데이터 로드하기(Data Load)
- 파이토치에서는 데이터를 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 데이터셋(Dataset)과 데이터로더(DataLoader)를 제공
- 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행
- Dataset을 정의하고, 이를 DataLoader에 전달

In [1]:
import torch
import torch.nn as nn
import torch.nn.functional as F

In [2]:
from torch.utils.data import TensorDataset # 텐서데이터셋
from torch.utils.data import DataLoader # 데이터로더

In [3]:
x_train  =  torch.FloatTensor([[73,  80,  75], 
                               [93,  88,  93], 
                               [89,  91,  90], 
                               [96,  98,  100],   
                               [73,  66,  70]])  
y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])

dataset = TensorDataset(x_train, y_train)

In [4]:
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

model = nn.Linear(3,1)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) 

nb_epochs = 20
for epoch in range(nb_epochs + 1):
  for batch_idx, samples in enumerate(dataloader):
    # print(batch_idx)
    # print(samples)
    x_train, y_train = samples
    # H(x) 계산
    prediction = model(x_train)

    # cost 계산
    cost = F.mse_loss(prediction, y_train)

    # cost로 H(x) 계산
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(
        epoch, nb_epochs, batch_idx+1, len(dataloader),
        cost.item()
        ))
		
Out[4]:
Epoch    0/20 Batch 1/3 Cost: 69164.390625
Epoch    0/20 Batch 2/3 Cost: 28381.636719
Epoch    0/20 Batch 3/3 Cost: 3707.019775
Epoch    1/20 Batch 1/3 Cost: 2479.864746
Epoch    1/20 Batch 2/3 Cost: 804.939575
Epoch    1/20 Batch 3/3 Cost: 391.793030
Epoch    2/20 Batch 1/3 Cost: 68.918716
Epoch    2/20 Batch 2/3 Cost: 11.397542
Epoch    2/20 Batch 3/3 Cost: 20.775352
Epoch    3/20 Batch 1/3 Cost: 2.593069
Epoch    3/20 Batch 2/3 Cost: 2.152452
Epoch    3/20 Batch 3/3 Cost: 0.401843
Epoch    4/20 Batch 1/3 Cost: 1.858748
Epoch    4/20 Batch 2/3 Cost: 0.459123
Epoch    4/20 Batch 3/3 Cost: 4.216225
Epoch    5/20 Batch 1/3 Cost: 1.424406
Epoch    5/20 Batch 2/3 Cost: 1.685652
Epoch    5/20 Batch 3/3 Cost: 1.128387
Epoch    6/20 Batch 1/3 Cost: 0.510620
Epoch    6/20 Batch 2/3 Cost: 2.303828
Epoch    6/20 Batch 3/3 Cost: 2.540088
Epoch    7/20 Batch 1/3 Cost: 2.021537
Epoch    7/20 Batch 2/3 Cost: 1.213806
Epoch    7/20 Batch 3/3 Cost: 1.251724
Epoch    8/20 Batch 1/3 Cost: 0.580489
Epoch    8/20 Batch 2/3 Cost: 3.971988
Epoch    8/20 Batch 3/3 Cost: 1.287416
Epoch    9/20 Batch 1/3 Cost: 0.486383
Epoch    9/20 Batch 2/3 Cost: 1.810234
Epoch    9/20 Batch 3/3 Cost: 3.213260
Epoch   10/20 Batch 1/3 Cost: 2.038324
Epoch   10/20 Batch 2/3 Cost: 1.786484
Epoch   10/20 Batch 3/3 Cost: 1.717973
Epoch   11/20 Batch 1/3 Cost: 2.475778
Epoch   11/20 Batch 2/3 Cost: 2.374422
Epoch   11/20 Batch 3/3 Cost: 2.217480
Epoch   12/20 Batch 1/3 Cost: 1.982338
Epoch   12/20 Batch 2/3 Cost: 0.639150
Epoch   12/20 Batch 3/3 Cost: 3.076829
Epoch   13/20 Batch 1/3 Cost: 1.950365
Epoch   13/20 Batch 2/3 Cost: 1.937436
Epoch   13/20 Batch 3/3 Cost: 1.878936
Epoch   14/20 Batch 1/3 Cost: 1.371691
Epoch   14/20 Batch 2/3 Cost: 1.630328
Epoch   14/20 Batch 3/3 Cost: 2.088619
Epoch   15/20 Batch 1/3 Cost: 0.313896
Epoch   15/20 Batch 2/3 Cost: 2.287000
Epoch   15/20 Batch 3/3 Cost: 2.584688
Epoch   16/20 Batch 1/3 Cost: 1.163614
Epoch   16/20 Batch 2/3 Cost: 1.950157
Epoch   16/20 Batch 3/3 Cost: 3.438575
Epoch   17/20 Batch 1/3 Cost: 1.701870
Epoch   17/20 Batch 2/3 Cost: 1.611721
Epoch   17/20 Batch 3/3 Cost: 2.417433
Epoch   18/20 Batch 1/3 Cost: 1.165142
Epoch   18/20 Batch 2/3 Cost: 1.956720
Epoch   18/20 Batch 3/3 Cost: 1.412041
Epoch   19/20 Batch 1/3 Cost: 1.430396
Epoch   19/20 Batch 2/3 Cost: 1.743238
Epoch   19/20 Batch 3/3 Cost: 1.329937
Epoch   20/20 Batch 1/3 Cost: 3.549675
Epoch   20/20 Batch 2/3 Cost: 1.727808
Epoch   20/20 Batch 3/3 Cost: 0.369295

In [5]:
# 임의의 입력 [73, 80, 75]를 선언
new_var =  torch.FloatTensor([[73, 80, 75]]) 
# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장
pred_y = model(new_var) 
print("훈련 후 입력이 73, 80, 75일 때의 예측값 :", pred_y) 

Out[5]:
훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[152.3968]], grad_fn=<AddmmBackward0>)

4) 커스텀 데이터셋(Custom Dataset)
기본 뼈대
class CustomDataset(torch.utils.data.Dataset): 
  def __init__(self):
  데이터셋의 전처리를 해주는 부분

  def __len__(self):
  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분

  def __getitem__(self, idx): 
  데이터셋에서 특정 1개의 샘플을 가져오는 함수
  
- len(dataset)을 했을 때 데이터셋의 크기를 리턴할 len
- dataset[i]을 했을 때 i번째 샘플을 가져오도록 하는 인덱싱을 위한 get_item

5) 커스텀 데이터셋(Custom Dataset)으로 선형 회귀 구현하기
In [1]:
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

In [2]:
# Dataset 상속
class CustomDataset(Dataset): 
  def __init__(self):
    self.x_data = [[73, 80, 75],
                   [93, 88, 93],
                   [89, 91, 90],
                   [96, 98, 100],
                   [73, 66, 70]]
    self.y_data = [[152], [185], [180], [196], [142]]

  # 총 데이터의 개수를 리턴
  def __len__(self): 
    return len(self.x_data)

  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴
  def __getitem__(self, idx): 
    x = torch.FloatTensor(self.x_data[idx])
    y = torch.FloatTensor(self.y_data[idx])
    return x, y
	
In [3]:
dataset = CustomDataset()
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

In [4]:
model = torch.nn.Linear(3,1)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) 

In [5]:
nb_epochs = 20
for epoch in range(nb_epochs + 1):
  for batch_idx, samples in enumerate(dataloader):
    # print(batch_idx)
    # print(samples)
    x_train, y_train = samples
    # H(x) 계산
    prediction = model(x_train)

    # cost 계산
    cost = F.mse_loss(prediction, y_train)

    # cost로 H(x) 계산
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(
        epoch, nb_epochs, batch_idx+1, len(dataloader),
        cost.item()
        ))
		
Out[5]:
Epoch    0/20 Batch 1/3 Cost: 18198.035156
Epoch    0/20 Batch 2/3 Cost: 6336.257812
Epoch    0/20 Batch 3/3 Cost: 2973.950195
Epoch    1/20 Batch 1/3 Cost: 583.830688
Epoch    1/20 Batch 2/3 Cost: 89.755478
Epoch    1/20 Batch 3/3 Cost: 33.413326
Epoch    2/20 Batch 1/3 Cost: 24.675480
Epoch    2/20 Batch 2/3 Cost: 9.089052
Epoch    2/20 Batch 3/3 Cost: 0.791447
Epoch    3/20 Batch 1/3 Cost: 0.191709
Epoch    3/20 Batch 2/3 Cost: 9.189552
Epoch    3/20 Batch 3/3 Cost: 7.177863
Epoch    4/20 Batch 1/3 Cost: 6.159242
Epoch    4/20 Batch 2/3 Cost: 4.012930
Epoch    4/20 Batch 3/3 Cost: 2.179111
Epoch    5/20 Batch 1/3 Cost: 1.212268
Epoch    5/20 Batch 2/3 Cost: 4.730374
Epoch    5/20 Batch 3/3 Cost: 12.121630
Epoch    6/20 Batch 1/3 Cost: 3.602578
Epoch    6/20 Batch 2/3 Cost: 11.145040
Epoch    6/20 Batch 3/3 Cost: 1.438675
Epoch    7/20 Batch 1/3 Cost: 6.582220
Epoch    7/20 Batch 2/3 Cost: 3.046957
Epoch    7/20 Batch 3/3 Cost: 7.943752
Epoch    8/20 Batch 1/3 Cost: 4.319757
Epoch    8/20 Batch 2/3 Cost: 7.121061
Epoch    8/20 Batch 3/3 Cost: 4.270035
Epoch    9/20 Batch 1/3 Cost: 6.053365
Epoch    9/20 Batch 2/3 Cost: 3.685191
Epoch    9/20 Batch 3/3 Cost: 10.211485
Epoch   10/20 Batch 1/3 Cost: 3.942950
Epoch   10/20 Batch 2/3 Cost: 5.407682
Epoch   10/20 Batch 3/3 Cost: 11.876067
Epoch   11/20 Batch 1/3 Cost: 5.240442
Epoch   11/20 Batch 2/3 Cost: 6.611593
Epoch   11/20 Batch 3/3 Cost: 3.600300
Epoch   12/20 Batch 1/3 Cost: 2.759129
Epoch   12/20 Batch 2/3 Cost: 6.650782
Epoch   12/20 Batch 3/3 Cost: 4.648888
Epoch   13/20 Batch 1/3 Cost: 1.958174
Epoch   13/20 Batch 2/3 Cost: 12.224627
Epoch   13/20 Batch 3/3 Cost: 2.583413
Epoch   14/20 Batch 1/3 Cost: 3.641631
Epoch   14/20 Batch 2/3 Cost: 6.551587
Epoch   14/20 Batch 3/3 Cost: 1.673663
Epoch   15/20 Batch 1/3 Cost: 1.013965
Epoch   15/20 Batch 2/3 Cost: 7.988110
Epoch   15/20 Batch 3/3 Cost: 5.705826
Epoch   16/20 Batch 1/3 Cost: 6.013357
Epoch   16/20 Batch 2/3 Cost: 2.512980
Epoch   16/20 Batch 3/3 Cost: 5.861414
Epoch   17/20 Batch 1/3 Cost: 3.642746
Epoch   17/20 Batch 2/3 Cost: 2.560634
Epoch   17/20 Batch 3/3 Cost: 14.706808
Epoch   18/20 Batch 1/3 Cost: 5.143028
Epoch   18/20 Batch 2/3 Cost: 9.381370
Epoch   18/20 Batch 3/3 Cost: 3.593181
Epoch   19/20 Batch 1/3 Cost: 6.539184
Epoch   19/20 Batch 2/3 Cost: 3.501682
Epoch   19/20 Batch 3/3 Cost: 1.466370
Epoch   20/20 Batch 1/3 Cost: 9.319826
Epoch   20/20 Batch 2/3 Cost: 5.189878
Epoch   20/20 Batch 3/3 Cost: 1.856997

In [6]:
# 임의의 입력 [73, 80, 75]를 선언
new_var =  torch.FloatTensor([[73, 80, 75]]) 
# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장
pred_y = model(new_var) 
print("훈련 후 입력이 73, 80, 75일 때의 예측값 :", pred_y) 

Out[6]:
훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[152.8897]], grad_fn=<AddmmBackward0>)