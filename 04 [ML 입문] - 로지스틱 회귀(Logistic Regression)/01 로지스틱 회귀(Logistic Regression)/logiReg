둘 중 하나를 결정하는 문제를 이진 분류(Binary Classification)
이진 분류를 풀기 위한 대표적인 알고리즘으로 로지스틱 회귀(Logistic Regression)

1) 이진 분류(Binary Classification)
분류 작업에서는 x와 y의 관계를 표현하기 위해서 Wx+b와 같은 직선 함수가 아니라 S자 형태로 표현할 수 있는 함수가 필요
로지스틱 회귀의 가설은 선형 회귀 때의 y = Wx+b가 아니라, 위와 같이 S자 모양의 그래프를 만들 수 있는 어떤 특정 함수 f를 추가적으로 사용하여 y = Wx+b의 가설을 사용, 시그모이드 함수라고 함수

2) 시그모이드 함수(Sigmoid function)
S자 형태로 그래프를 그려주는 시그모이드 함수의 방정식 - 
H(x) = sigmoid(Wx + b) = \frac{1}{1 + e^{-(Wx + b)}} = σ(Wx + b)

파이썬에서는 그래프를 그릴 수 있는 도구로서 Matplotlib을 사용.

In [1]:
# %matplotlib inline # 아나콘다에서 할때는 주석처리 필요
import numpy as np # 넘파이 사용
import matplotlib.pyplot as plt # 맷플롯립사용

In [2]:
def sigmoid(x): # 시그모이드 함수 정의
    return 1/(1+np.exp(-x))
	
- W가 1이고 b가 0인 그래프	
In [3]:
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)

plt.title('Sigmoid Function')
plt.plot(x, y, 'g')
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.show()

Out[3]:이미지(SigmoidFunction.png) 참조

In [4]:
x = np.arange(-5.0, 5.0, 0.1)
y1 = sigmoid(0.5*x)
y2 = sigmoid(x)
y3 = sigmoid(2*x)

plt.plot(x, y1, 'r', linestyle='--') # W의 값이 0.5일때
plt.plot(x, y2, 'g') # W의 값이 1일때
plt.plot(x, y3, 'b', linestyle='--') # W의 값이 2일때
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()

Out[4]:이미지(SigmoidFunction2.png) 참조

In [5]:
x = np.arange(-5.0, 5.0, 0.1)
y1 = sigmoid(x+0.5)
y2 = sigmoid(x+1)
y3 = sigmoid(x+1.5)

plt.plot(x, y1, 'r', linestyle='--') # x + 0.5
plt.plot(x, y2, 'g') # x + 1
plt.plot(x, y3, 'b', linestyle='--') # x + 1.5
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()

Out[5]:이미지(SigmoidFunction3.png) 참조

- 시그모이드 함수를 이용한 분류
시그모이드 함수는 입력값이 한없이 커지면 1에 수렴하고, 입력값이 한없이 작아지면 0에 수렴
시그모이드 함수의 출력값은 0과 1 사이의 값을 가지는데 이 특성을 이용하여 분류 작업에 사용

3) 비용 함수(Cost function)
최적의 W와 b를 찾을 수 있는 비용 함수(cost function)를 정의
선형 회귀에서 사용했던 평균 제곱 오차의 수식
cost(W, b) = \frac{1}{n} \sum_{i=1}^{n} \left[y^{(i)} - H(x^{(i)})\right]^2
비용 함수를 미분하면 선형 회귀때와 달리 다음의 그림(localMin.png)과 유사한 심한 비볼록(non-convex) 형태의 그래프
그래프에 경사 하강법을 사용할 경우의 문제점은 경사 하강법이 오차가 최소값이 되는 구간에 도착했다고 판단한 그 구간이 실제 오차가 완전히 최소값이 되는 구간이 아닐 수 있다는 점
모델도 마찬가지로 실제 오차가 최소가 되는 구간을 찾을 수 있도록 도와주어야 함
전체 함수에 걸쳐 최소값인 글로벌 미니멈(Global Minimum)이 아닌 특정 구역에서의 최소값인 로컬 미니멈(Local Minimum)에 도달
시그모이드 함수의 특징은 함수의 출력값이 0과 1사이의 값이라는 점
실제값이 1일 때 예측값이 0에 가까워지면 오차가 커져야 하며, 실제값이 0일 때, 예측값이 1에 가까워지면 오차가 커져야 함(이를 충족하는 함수가 로그 함수)

4) 파이토치로 로지스틱 회귀 구현하기
In [1]:
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

Out[1]: <torch._C.Generator at 0x18425bdd630>

In [2]:
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

In [3]:
print(x_train.shape)
print(y_train.shape)

Out[3]: 
torch.Size([6, 2])
torch.Size([6, 1])

In [4]:
W = torch.zeros((2, 1), requires_grad=True) # 크기는 2 x 1
b = torch.zeros(1, requires_grad=True)

In [5]:
hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))

In [6]:
print(hypothesis) # 예측값인 H(x) 출력

Out[6]: 
tensor([[0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000]], grad_fn=<MulBackward0>)

In [7]:
hypothesis = torch.sigmoid(x_train.matmul(W) + b)

In [8]:
print(hypothesis)

Out[8]: 
tensor([[0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000]], grad_fn=<SigmoidBackward0>)
		
cost 계산식
cost(W) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]

In [9]:
-(y_train[0] * torch.log(hypothesis[0]) + 
  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))
  
Out[9]:
tensor([0.6931], grad_fn=<NegBackward0>)

In [10]:
losses = -(y_train * torch.log(hypothesis) + 
           (1 - y_train) * torch.log(1 - hypothesis))
print(losses)

Out[10]:
tensor([[0.6931],
        [0.6931],
        [0.6931],
        [0.6931],
        [0.6931],
        [0.6931]], grad_fn=<NegBackward0>)

In [11]: 
F.binary_cross_entropy(hypothesis, y_train)

Out[11]: 
tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)

In [12]: 
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

# 모델 초기화
W = torch.zeros((2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# optimizer 설정
optimizer = optim.SGD([W, b], lr=1)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):

    # Cost 계산
    hypothesis = torch.sigmoid(x_train.matmul(W) + b)
    cost = -(y_train * torch.log(hypothesis) + 
             (1 - y_train) * torch.log(1 - hypothesis)).mean()

    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    # 100번마다 로그 출력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} Cost: {:.6f}'.format(
            epoch, nb_epochs, cost.item()
        ))

Out[12]:
Epoch    0/1000 Cost: 0.693147
Epoch  100/1000 Cost: 0.134722
Epoch  200/1000 Cost: 0.080643
Epoch  300/1000 Cost: 0.057900
Epoch  400/1000 Cost: 0.045300
Epoch  500/1000 Cost: 0.037261
Epoch  600/1000 Cost: 0.031673
Epoch  700/1000 Cost: 0.027556
Epoch  800/1000 Cost: 0.024394
Epoch  900/1000 Cost: 0.021888
Epoch 1000/1000 Cost: 0.019852

In [13]: 
hypothesis = torch.sigmoid(x_train.matmul(W) + b)
print(hypothesis)

Out[13]:
tensor([[2.7648e-04],
        [3.1608e-02],
        [3.8977e-02],
        [9.5622e-01],
        [9.9823e-01],
        [9.9969e-01]], grad_fn=<SigmoidBackward0>)
		
In [14]: 
prediction = hypothesis >= torch.FloatTensor([0.5])
print(prediction)

Out[14]:
tensor([[False],
        [False],
        [False],
        [ True],
        [ True],
        [ True]])

In [15]: 
print(W)
print(b)

Out[15]:
tensor([[3.2530],
        [1.5179]], requires_grad=True)
tensor([-14.4819], requires_grad=True)