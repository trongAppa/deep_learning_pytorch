딥 러닝 또한 머신 러닝에 속하므로, 아래의 머신 러닝의 특징들은 모두 딥 러닝의 특징

1) 머신 러닝 모델의 평가
실제 모델을 평가하기 위해서 데이터를 훈련용, 검증용, 테스트용 이렇게 세 가지로 분리하는 것이 일반적
개념 학습이므로 일부 실습에서는 별도로 세 가지로 분리하지 않고 훈련용, 테스트용으로만 분리해서 사용
훈련용, 테스트용 두 가지로만 나눠서 테스트 데이터로 한 번만 테스트하면 더 편할텐데 굳이 왜 검증용 데이터를 만들어 놓는 것

검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라, 모델의 성능을 조정하기 위한 용도
더 정확히는 과적합이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도
하이퍼파라미터(초매개변수)란 값에 따라서 모델의 성능에 영향을 주는 매개변수들을 지칭
가중치와 편향과 같은 학습을 통해 바뀌어져가는 변수를 이 책에서는 매개변수

이 두 값 하이퍼파라미터와 매개변수의 가장 큰 차이는 하이퍼파라미터는 보통 사용자가 직접 정해줄 수 있는 변수라는 점
선형 회귀 챕터에서 배우게 되는 경사 하강법에서 학습률(learning rate)이 이에 해당되며 딥 러닝에서는 은닉층의 수, 뉴런의 수, 드롭아웃 비율 등이 이에 해당
매개변수는 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값

훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 튜닝(tuning)
이 모델의 매개변수는 검증용 데이터로 정확도가 검증되는 과정에서 점차 검증용 데이터에 점점 맞추어져 가기 시작

하이퍼파라미터 튜닝이 끝났다면, 이제 검증용 데이터로 모델을 평가하는 것은 적합하지 않음
모델은 검증용 데이터에 대해서도 일정 부분 최적화가 되어있기 때문
모델에 대한 평가는 모델이 아직까지 보지 못한 데이터로 하는 것이 가장 바람직
검증이 끝났다면 테스트 데이터를 가지고 모델의 진짜 성능을 평가
비유하자면 훈련 데이터는 문제지, 검증 데이터는 모의고사, 테스트 데이터는 실력을 최종적으로 평가하는 수능 시험이라고 볼 수 있음

검증 데이터와 테스트 데이터를 나눌 만큼 데이터가 충분하지 않다면 k-폴드 교차 검증이라는 또 다른 방법을 사용하기도 함

2) 분류(Classification)와 회귀(Regression)
전부라고는 할 수 없지만, 머신 러닝의 많은 문제는 분류 또는 회귀 문제에 속함
앞서 머신 러닝 기법 중 선형 회귀(Lineare Regression)과 로지스틱 회귀(Logistic Rgression)를 다루는데 선형 회귀를 통해 회귀 문제에 대해서 학습하고, 로지스틱 회귀를 통해 (이름은 회귀이지만) 분류 문제를 학습

분류는 또한 이진 분류(Binary Classification)과 다중 클래스 분류(Multi-Class Classification)로 나뉨

1. 이진 분류 문제(Binary Classification)
주어진 입력에 대해서 둘 중 하나의 답을 정하는 문제

2. 다중 클래스 분류(Multi-class Classification)
주어진 입력에 대해서 세 개 이상의 정해진 선택지 중에서 답을 정하는 문제
다섯 개의 선택지를 주로 카테고리 또는 범주 또는 클래스라고 하며, 주어진 입력으로부터 정해진 클래스 중 하나로 판단하는 것을 다중 클래스 분류 문제

3. 회귀 문제(Regression)
분류 문제처럼 0 또는 1이나 과학 책장, IT 책장 등과 같이 분리된(비연속적인) 답이 결과가 아니라 연속된 값을 결과
시계열 데이터를 이용한 주가 예측, 생산량 예측, 지수 예측 등이 이에 속함

3) 지도 학습(Supervised Learning)과 비지도 학습(Unsupervised Learning)
1. 지도 학습
레이블(Label)이라는 정답과 함께 학습하는 것
예측값과 실제값의 차이인 오차를 줄이는 방식으로 학습

2. 비지도 학습
기본적으로 목적 데이터(또는 레이블)이 없는 학습 방법
대표적으로 군집(clustering)이나 차원 축소와 같은 학습 방법들을 비지도 학습

3. 강화 학습
어떤 환경 내에서 정의된 에이전트가 현재의 상태를 인식하여, 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법

4) 샘플(Sample)과 특성(Feature)
인공 신경망을 배우게되면 훈련 데이터를 행렬로 표현하는 경우를 많음
독립 변수 x의 행렬을 X라고 하였을 때, 독립 변수의 개수가 n개이고 데이터의 개수가 m인 행렬 X는 그림(feature.png)과 같음

이때 머신 러닝에서는 하나의 데이터, 하나의 행을 샘플(Sample) - 데이터베이스에서는 레코드라고 부르는 단위
종속 변수 y를 예측하기 위한 각각의 독립 변수 x는 특성(Feature)

5) 혼동 행렬(Confusion Matrix)
맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)
하지만 정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않음, 이를 위해서 사용하는 것이 혼동 행렬(Confusion Matrix)
양성(Positive)과 음성(Negative)을 구분하는 이진 분류가 있다고 하였을 때 혼동 행렬(참&참:TP(True Positive) / 참&거짓:FN(False Negative) / 거짓&참:FP(False Postivie) / 거짓&거짓:TN(True Negative))
True는 정답을 맞춘 경우고 False는 정답을 맞추지 못한 경우
Positive와 Negative는 각각 제시했던 정답
TP는 양성(Postive)이라고 대답하였고 실제로 양성이라서 정답을 맞춘 경우
TN은 음성(Negative)이라고 대답하였는데 실제로 음성이라서 정답을 맞춘 경우
FP는 양성이라고 대답하였는데, 음성이라서 정답을 틀린 경우이며 FN은 음성이라고 대답하였는데 양성이라서 정답을 틀린 경우

이 개념을 사용하면 또 새로운 개념인 정밀도(Precision)과 재현률(Recall)

1. 정밀도(Precision)
양성이라고 대답한 전체 케이스에 대한 TP의 비율
Precision = \frac{TP}{TP + FP}

2. 재현률(Recall)
실제값이 양성인 데이터의 전체 개수에 대해서 TP의 비율
양성인 데이터 중에서 얼마나 양성인지를 예측(재현)했는지
Recall = \frac{TP}{TP + FN}

6) 과적합(Overfitting)과 과소 적합(Underfitting)
과적합(Overfitting) : 훈련 데이터를 과하게 학습한 경우, 기계가 훈련 데이터에 대해서만 과하게 학습하면 테스트 데이터나 실제 서비스에서의 데이터에 대해서는 정확도가 좋지 않은 현상이 발생

예를 들어 강아지 사진과 고양이 사진을 구분하는 기계가 있을 때, 검은색 강아지 사진 훈련 데이터를 과하게 학습하면 기계는 나중에 가서는 흰색 강아지나, 갈색 강아지를 보고도 강아지가 아니라고 판단하게 됩니다. 이는 훈련 데이터에 대해서 지나친 일반화를 한 상황

과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 높아지는 상황이 발생
과적합 상황에서 발생할 수 있는 훈련 횟수에 따른 훈련 데이터의 오차와 테스트 데이터의 오차의 변화를 보여줌(modelLoss.png)

X축의 에포크(epoch)는 전체 훈련 데이터에 대한 훈련 횟수를 의미, 에포크가 3~4를 넘어가게 되면 과적합이 발생, 테스트 데이터에 대한 오차가 점차 증가하는 양상
훈련 데이터에 대한 정확도는 높지만, 테스트 데이터는 정확도가 낮은 상황
테스트 데이터의 오차가 증가하기 전이나, 정확도가 감소하기 전에 훈련을 멈추는 것이 바람직함

과소적합(Underfitting) : 과적합 방지를 위해 테스트 데이터의 성능이 낮아지기 전에 훈련을 멈추는 것이 바람직하지만 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태를 반대, 훈련 자체가 부족한 상태이므로 과대 적합과는 달리 훈련 데이터에 대해서도 보통 정확도가 낮다는 특징

적합(fitting) : 두 가지 현상을 과적합과 과소 적합이라고 부르는 이유는 머신 러닝에서 학습 또는 훈련이라고 하는 과정, 모델이 주어진 데이터에 대해서 적합해져가는 과정

딥 러닝을 할 때는 과적합을 막을 수 있는 드롭아웃(Dropout), 조기 종료(Early Stopping)과 같은 몇 가지 방법이 존재

7) 비선형 활성화 함수
입력을 받아 수학적 변환을 수행하고 출력을 생성하는 함수
앞서 배운 시그모이드 함수나 소프트맥스 함수는 대표적인 활성화 함수 중 하나

필요 import
In [1]:
import numpy as np # 넘파이 사용
import matplotlib.pyplot as plt # 맷플롯립 사용

1. 활성화 함수의 특징 - 비선형 함수(Nonlinear function)
선형 함수가 아닌 비선형 함수여야 한다는 점
인공 신경망의 능력을 높이기 위해서는 은닉층을 계속해서 추가
선형 함수를 사용한 은닉층을 1회 추가한 것과 연속으로 추가한 것이 차이가 없다는 뜻이지, 선형 함수를 사용한 층이 아무 의미가 없다는 뜻이 아님
활성화 함수를 사용하는 일반적인 은닉층을 선형층과 대비되는 표현을 사용하면 비선형층(nonlinear layer)

2. 시그모이드 함수(Sigmoid function)와 기울기 소실
인공 신경망의 학습 과정(learn.png)
인공 신경망은 입력에 대해서 순전파(forward propagation) 연산 -> 순전파 연산을 통해 나온 예측값과 실제값의 오차를 손실 함수(loss function)을 통해 계산 -> 이 손실(loss)을 미분을 통해서 기울기(gradient)를 구함 -> 역전파(back propagation)를 수행

시그모이드 함수의 문제점은 미분을 해서 기울기(gradient)를 구할 때 발생
In [2]:
# 시그모이드 함수 그래프를 그리는 코드
def sigmoid(x):
    return 1/(1+np.exp(-x))
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)

plt.plot(x, y)
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()

기울기를 계산하면 0에 가까운 아주 작은 값이 나오게 됨(-3>=x or 3<=x) 구간
기울기 소실(Vanishing Gradient) 문제 : 역전파 과정에서 0에 가까운 아주 작은 기울기가 곱해지게 되면, 앞단에는 기울기가 잘 전달되지 않게 되는 문제

시그모이드 함수를 사용하는 은닉층의 개수가 다수가 될 경우에는 0에 가까운 기울기가 계속 곱해지면 앞단에서는 거의 기울기를 전파받을 수 없게 됨
다시 말해 매개변수 W가 업데이트 되지 않아 학습이 되지를 않음

은닉층이 깊은 신경망에서 기울기 소실 문제로 인해 출력층과 가까운 은닉층에서는 기울기가 잘 전파되지만, 앞단으로 갈수록 기울기가 제대로 전파되지 않는 모습을 보여줌
시그모이드 함수를 은닉층에서 사용하는 것은 지양

3. 하이퍼볼릭탄젠트 함수(Hyperbolic tangent function)
하이퍼볼릭탄젠트 함수(tanh)는 입력값을 -1과 1사이의 값으로 변환

In [3]:
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = np.tanh(x)

plt.plot(x, y)
plt.plot([0,0],[1.0,-1.0], ':')
plt.axhline(y=0, color='orange', linestyle='--')
plt.title('Tanh Function')
plt.show()

시그모이드 함수와는 달리 0을 중심으로 하고 있는데, 이때문에 시그모이드 함수와 비교하면 반환값의 변화폭이 더 큼
시그모이드 함수보다는 기울기 소실 증상이 적은 편
은닉층에서 시그모이드 함수보다는 많이 사용

4. 렐루 함수(ReLU)
인공 신경망에서 가장 최고의 인기를 얻고 있는 함수
수식은 아주 간단 f(x) = max(0, x)

In [4]:
def relu(x):
    return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Relu Function')
plt.show()

음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환
특정 양수값에 수렴하지 않으므로 깊은 신경망에서 시그모이드 함수보다 훨씬 더 잘 작동
시그모이드 함수와 하이퍼볼릭탄젠트 함수와 같이 어떤 연산이 필요한 것이 아니라 단순 임계값이므로 연산 속도도 빠름

죽은 렐루(dying ReLU) : 입력값이 음수면 기울기도 0, 뉴런은 다시 회생하는 것이 매우 어려움

5. 리키 렐루(Leaky ReLU)
죽은 렐루를 보완하기 위해 ReLU의 변형 함수들이 등장하기 시작
입력값이 음수일 경우에 0이 아니라 0.001과 같은 매우 작은 수를 반환
수식 : f(x) = max(ax, x) | a는 하이퍼파라미터로 Leaky('새는') 정도를 결정하며 일반적으로는 0.01의 값

In [5]:
a = 0.1

def leaky_relu(x):
    return np.maximum(a*x, x)

x = np.arange(-5.0, 5.0, 0.1)
y = leaky_relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Leaky ReLU Function')
plt.show()

입력값이 음수라도 기울기가 0이 되지 않으면 ReLU는 죽지않음

6. 소프트맥스 함수(Softamx function)
분류 문제를 로지스틱 회귀와 소프트맥스 회귀를 출력층에 적용하여 사용

In [6]:
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = np.exp(x) / np.sum(np.exp(x))

plt.plot(x, y)
plt.title('Softmax Function')
plt.show()

시그모이드 함수처럼 출력층의 뉴런에서 주로 사용되는데, 시그모이드 함수가 두 가지 선택지 중 하나를 고르는 이진 분류 (Binary Classification) 문제에 사용된다면 세 가지 이상의 (상호 배타적인) 선택지 중 하나를 고르는 다중 클래스 분류(MultiClass Classification) 문제에 주로 사용

7. 출력층의 활성화 함수와 오차 함수의 관계
문제에 따른 출력층의 활성화 함수와 비용 함수의 관계를 정리
문제         | 활성화 함수| 비용 함수
이진 분류     | 시그모이드  | nn.BCELoss()
다중 클래스 분류| 소프트맥스  | nn.CrossEntropyLoss()
회귀	        | 없음      |	MSE

nn.CrossEntropyLoss()는 소프트맥스 함수를 이미 포함

시그모이드 함수 문제점은 원점 중심이 아니라는 점(Not zero-centered)
평균이 0이 아니라 0.5이며, 시그모이드 함수는 항상 양수를 출력하기 때문에 출력의 가중치 합이 입력의 가중치 합보다 커질 가능성이 높음
각 레이어를 지날 때마다 분산이 계속 커져 가장 높은 레이어에서는 활성화 함수의 출력이 0이나 1로 수렴하게 되어 기울기 소실 문제가 발생할 수 있음

하이퍼볼릭탄젠트 함수는 원점 중심(zero-centered)이기 때문에, 시그모이드와 달리 편향 이동은 일어나지 않음
입력의 절대값이 클 경우 -1이나 1로 수렴하게 되는데 시그모이드 함수와 마찬가지로 이때 기울기가 완만해지므로 역시나 기울기 소실 문제가 일어날 수 있음

스탠포드 대학교의 딥 러닝 강의 cs231n에서는 ReLU를 먼저 시도해보고, 그다음으로 LeakyReLU나 ELU 같은 ReLU의 변형들을 시도해보며, sigmoid는 사용하지 말라고 권장

