1. 파이토치로 단층 퍼셉트론 구현하기
In [1]: 
import torch
import torch.nn as nn
device = 'cuda' if torch.cuda.is_available() else 'cpu'
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
	
1) 단층 퍼셉트론을 이용한 XOR 문제 풀기
XOR 문제에 해당되는 입력과 출력을 정의

In [2]:
X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)
Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)

1개의 뉴런을 가지는 시그모이드 함수를 사용하여 단층 퍼셉트론을 구현 

In [3]:
linear = nn.Linear(2, 1, bias=True)
sigmoid = nn.Sigmoid()
model = nn.Sequential(linear, sigmoid).to(device)

0 또는 1을 예측하는 이진 분류 문제이므로 비용 함수로는 크로스엔트로피 함수를 사용
nn.BCELoss()는 이진 분류에서 사용하는 크로스엔트로피 함수

In [4]:
# 비용 함수와 옵티마이저 정의
criterion = torch.nn.BCELoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=1)
for step in range(10001): 
    optimizer.zero_grad()
    hypothesis = model(X)

    # 비용 함수
    cost = criterion(hypothesis, Y)
    cost.backward()
    optimizer.step()

    if step % 100 == 0: # 100번째 에포크마다 비용 출력
        print(step, cost.item())

Out[4]:
0 0.7273974418640137
100 0.6931476593017578
200 0.6931471824645996
300 0.6931471824645996
400 0.6931471824645996
500 0.6931471824645996
600 0.6931471824645996
700 0.6931471824645996
800 0.6931471824645996
900 0.6931471824645996
1000 0.6931471824645996
1100 0.6931471824645996
1200 0.6931471824645996
1300 0.6931471824645996
1400 0.6931471824645996
1500 0.6931471824645996
1600 0.6931471824645996
1700 0.6931471824645996
1800 0.6931471824645996
1900 0.6931471824645996
2000 0.6931471824645996
2100 0.6931471824645996
2200 0.6931471824645996
2300 0.6931471824645996
2400 0.6931471824645996
2500 0.6931471824645996
2600 0.6931471824645996
2700 0.6931471824645996
2800 0.6931471824645996
2900 0.6931471824645996
3000 0.6931471824645996
3100 0.6931471824645996
3200 0.6931471824645996
3300 0.6931471824645996
3400 0.6931471824645996
3500 0.6931471824645996
3600 0.6931471824645996
3700 0.6931471824645996
3800 0.6931471824645996
3900 0.6931471824645996
4000 0.6931471824645996
4100 0.6931471824645996
4200 0.6931471824645996
4300 0.6931471824645996
4400 0.6931471824645996
4500 0.6931471824645996
4600 0.6931471824645996
4700 0.6931471824645996
4800 0.6931471824645996
4900 0.6931471824645996
5000 0.6931471824645996
5100 0.6931471824645996
5200 0.6931471824645996
5300 0.6931471824645996
5400 0.6931471824645996
5500 0.6931471824645996
5600 0.6931471824645996
5700 0.6931471824645996
5800 0.6931471824645996
5900 0.6931471824645996
6000 0.6931471824645996
6100 0.6931471824645996
6200 0.6931471824645996
6300 0.6931471824645996
6400 0.6931471824645996
6500 0.6931471824645996
6600 0.6931471824645996
6700 0.6931471824645996
6800 0.6931471824645996
6900 0.6931471824645996
7000 0.6931471824645996
7100 0.6931471824645996
7200 0.6931471824645996
7300 0.6931471824645996
7400 0.6931471824645996
7500 0.6931471824645996
7600 0.6931471824645996
7700 0.6931471824645996
7800 0.6931471824645996
7900 0.6931471824645996
8000 0.6931471824645996
8100 0.6931471824645996
8200 0.6931471824645996
8300 0.6931471824645996
8400 0.6931471824645996
8500 0.6931471824645996
8600 0.6931471824645996
8700 0.6931471824645996
8800 0.6931471824645996
8900 0.6931471824645996
9000 0.6931471824645996
9100 0.6931471824645996
9200 0.6931471824645996
9300 0.6931471824645996
9400 0.6931471824645996
9500 0.6931471824645996
9600 0.6931471824645996
9700 0.6931471824645996
9800 0.6931471824645996
9900 0.6931471824645996
10000 0.6931471824645996

200번 에포크에 비용이 0.6931471824645996가 출력된 이후에는 10,000번 에포크가 되는 순간까지 더 이상 비용이 줄어들지 않음
단층 퍼셉트론은 XOR 문제를 풀 수 없음

2) 학습된 단층 퍼셉트론의 예측값 확인하기
총 10,001회 학습한 단층 퍼셉트론의 예측값 확인

In [5]:
with torch.no_grad():
    hypothesis = model(X)
    predicted = (hypothesis > 0.5).float()
    accuracy = (predicted == Y).float().mean()
    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())
    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())
    print('실제값(Y): ', Y.cpu().numpy())
    print('정확도(Accuracy): ', accuracy.item())
	
Out[5]:
모델의 출력값(Hypothesis):  [[0.5]
 [0.5]
 [0.5]
 [0.5]]
모델의 예측값(Predicted):  [[0.]
 [0.]
 [0.]
 [0.]]
실제값(Y):  [[0.]
 [1.]
 [1.]
 [0.]]
정확도(Accuracy):  0.5
	
with torch.no_grad(): 블록 안에서는 기울기 계산을 비활성화 / 연산 속도를 높이고 메모리 사용을 줄임 / 학습이 아니라 모델의 성능을 평가하는 단계이기 때문에 기울기 계산이 불 필요

hypothesis = model(X)
입력 데이터 X에 대한 모델의 예측값을 계산 / 모델의 출력값으로, 0과 1 사이의 확률을 나타냄

predicted = (hypothesis > 0.5).float()
예측값이 0.5를 초과하면 1, 그렇지 않으면 0으로 간주하여 이진 분류를 수행 / 예측한 클래스 레이블

accuracy = (predicted == Y).float().mean()
모델의 예측값과 실제값 Y를 비교하고, 그 일치하는 비율을 계산하여 정확도 계산

모델의 출력값(hypothesis), 모델의 이진 분류 예측값(predicted), 실제값(Y), 그리고 정확도(accuracy)를 출력
각 값은 .detach().cpu().numpy()를 사용해 PyTorch 텐서를 NumPy 배열로 변환하여 출력하기 전에 계산 그래프에서 분리하고 CPU로 옮김

실제값은 0, 1, 1, 0임에도 예측값은 0, 0, 0, 0으로 문제를 풀지 못하는 모습을 보여줌

2. 파이토치로 다층 퍼셉트론 구현하기

In [1]:
import torch
import torch.nn as nn
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# for reproducibility
torch.manual_seed(777)
if device == 'cuda':
    torch.cuda.manual_seed_all(777)
	
1) 다층 퍼셉트론을 이용한 XOR 문제 풀기
XOR 문제를 풀기 위한 입력과 출력을 정의
In [2]:
X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)
Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)

다층 퍼셉트론을 설계
입력층, 은닉층1, 은닉층2, 은닉층3, 출력층을 가지는 은닉층이 3개인 인공 신경망

In [3]:
model = nn.Sequential(
          nn.Linear(2, 10, bias=True), # input_layer = 2, hidden_layer1 = 10
          nn.Sigmoid(),
          nn.Linear(10, 10, bias=True), # hidden_layer1 = 10, hidden_layer2 = 10
          nn.Sigmoid(),
          nn.Linear(10, 10, bias=True), # hidden_layer2 = 10, hidden_layer3 = 10
          nn.Sigmoid(),
          nn.Linear(10, 1, bias=True), # hidden_layer3 = 10, output_layer = 1
          nn.Sigmoid()
          ).to(device)

인공 신경망을 그림으로 표현(ann.png)

비용 함수와 옵타마이저를 선언
nn.BCELoss()는 이진 분류에서 사용하는 크로스엔트로피 함수

In [4]:
criterion = torch.nn.BCELoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1

In [5]:
for epoch in range(10001):
    optimizer.zero_grad()
    # forward 연산
    hypothesis = model(X)

    # 비용 함수
    cost = criterion(hypothesis, Y)
    cost.backward()
    optimizer.step()

    # 100의 배수에 해당되는 에포크마다 비용을 출력
    if epoch % 100 == 0:
        print(epoch, cost.item())
		
Out[5]:
0 0.6948983669281006
100 0.693155825138092
200 0.6931535601615906
300 0.6931513547897339
400 0.693149209022522
500 0.6931473016738892
600 0.6931453943252563
700 0.6931434273719788
800 0.6931416988372803
900 0.6931397914886475
1000 0.6931380033493042
1100 0.6931361556053162
1200 0.6931343078613281
1300 0.6931324005126953
1400 0.6931304931640625
1500 0.6931284666061401
1600 0.6931264400482178
1700 0.6931242942810059
1800 0.6931220889091492
1900 0.6931197047233582
2000 0.6931171417236328
2100 0.6931145191192627
2200 0.6931115984916687
2300 0.6931084990501404
2400 0.6931051015853882
2500 0.6931014657020569
2600 0.6930974721908569
2700 0.6930930018424988
2800 0.6930879950523376
2900 0.6930825114250183
3000 0.6930763721466064
3100 0.6930692195892334
3200 0.6930612325668335
3300 0.6930519342422485
3400 0.6930410861968994
3500 0.6930283904075623
3600 0.6930132508277893
3700 0.6929951310157776
3800 0.6929728984832764
3900 0.6929453015327454
4000 0.6929103136062622
4100 0.6928649544715881
4200 0.6928046941757202
4300 0.692721962928772
4400 0.692604124546051
4500 0.6924278736114502
4600 0.6921480298042297
4700 0.6916664838790894
4800 0.6907395124435425
4900 0.6886203289031982
5000 0.6820818185806274
5100 0.6472527980804443
5200 0.44994843006134033
5300 0.041418157517910004
5400 0.009738397784531116
5500 0.005035833455622196
5600 0.0032971729524433613
5700 0.0024168307427316904
5800 0.0018922279123216867
5900 0.0015467405319213867
6000 0.0013033139985054731
6100 0.0011231712996959686
6200 0.0009848718764260411
6300 0.0008755262824706733
6400 0.000787071418017149
6500 0.0007141333189792931
6600 0.0006530550308525562
6700 0.0006011504447087646
6800 0.0005565693136304617
6900 0.0005178644205443561
7000 0.00048400633386336267
7100 0.0004541150410659611
7200 0.00042753416346386075
7300 0.0004037863982375711
7400 0.0003824243613053113
7500 0.0003631050349213183
7600 0.00034560466883704066
7700 0.00032958033261820674
7800 0.00031495740404352546
7900 0.00030152720864862204
8000 0.0002891554613597691
8100 0.0002777229528874159
8200 0.00026711029931902885
8300 0.000257257983321324
8400 0.0002480764815118164
8500 0.00023949125898070633
8600 0.0002314575540367514
8700 0.0002239307068521157
8800 0.0002168510400224477
8900 0.00021021856809966266
9000 0.00020391402358654886
9100 0.0001979821245186031
9200 0.00019239308312535286
9300 0.00018708722200244665
9400 0.00018206457025371492
9500 0.00017725062207318842
9600 0.00017273474077228457
9700 0.00016841263277456164
9800 0.00016429921379312873
9900 0.00016036465240176767
10000 0.0001565940328873694

비용이 최소화 되는 방향으로 가중치와 편향이 업데이트
100배수의 에포크마다 비용이 줄어듬

2) 학습된 다층 퍼셉트론의 예측값 확인하기
모델이 XOR 문제를 풀 수 있는지 테스트

In [6]
with torch.no_grad():
    hypothesis = model(X)
    predicted = (hypothesis > 0.5).float()
    accuracy = (predicted == Y).float().mean()
    print('모델의 출력값(Hypothesis): ', hypothesis.detach().cpu().numpy())
    print('모델의 예측값(Predicted): ', predicted.detach().cpu().numpy())
    print('실제값(Y): ', Y.cpu().numpy())
    print('정확도(Accuracy): ', accuracy.item())
	
Out[6]:
모델의 출력값(Hypothesis):  [[1.11739784e-04]
 [9.99828696e-01]
 [9.99842167e-01]
 [1.85383164e-04]]
모델의 예측값(Predicted):  [[0.]
 [1.]
 [1.]
 [0.]]
실제값(Y):  [[0.]
 [1.]
 [1.]
 [0.]]
정확도(Accuracy):  1.0

with torch.no_grad()
블록 안에서는 기울기 계산을 비활성화하여 연산 속도를 높이고 메모리 사용을 줄임 / 모델의 성능을 평가하는 단계이기 때문에 기울기 계산이 필요하지 않음

hypothesis = model(X)
데이터 X에 대한 모델의 예측값을 계산 / 모델의 출력값으로, 0과 1 사이의 확률을 나타냄

predicted = (hypothesis > 0.5).float()
예측값이 0.5를 초과하면 1, 그렇지 않으면 0으로 간주하여 이진 분류를 수행 / 예측한 클래스 레이블

accuracy = (predicted == Y).float().mean()
모델의 예측값과 실제값 Y를 비교하고, 그 일치하는 비율을 계산하여 정확도를 구함

모델의 출력값(hypothesis), 모델의 이진 분류 예측값(predicted), 실제값(Y), 그리고 정확도(accuracy)를 출력
.detach().cpu().numpy()를 사용해 PyTorch 텐서를 NumPy 배열로 변환하여 출력하기 전에 계산 그래프에서 분리하고 CPU로 옮김

실제값은 0, 1, 1, 0이며 예측값은 0, 1, 1, 0으로 문제를 해결하는 모습을 보여줍니다.

인공 신경망 그림 그리기 : http://alexlenail.me/NN-SVG/index.html